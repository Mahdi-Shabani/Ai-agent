{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahdi-Shabani/Ai-agent/blob/master/Project01_DS04_S01_NLTK_SpaCy_RezaShokrzad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“œ Project: Job Description Analyzer â€“ Extracting Required Skills from Job Postings\n"
      ],
      "metadata": {
        "id": "1y3mR1nVT1zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Œ Objective\n",
        "Use spaCyâ€™s Named Entity Recognition (NER) and NLTK preprocessing to extract and categorize required skills from job descriptions. The goal is to identify trends in job requirements and analyze the most in-demand skills across industries."
      ],
      "metadata": {
        "id": "JSI2d6TlT67t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ› ï¸ Project Steps & Instructions\n"
      ],
      "metadata": {
        "id": "bEMWB0ErT-VI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ðŸ“¥ Download the Dataset\n",
        "!wget https://raw.githubusercontent.com/binoydutt/Resume-Job-Description-Matching/refs/heads/master/data.csv"
      ],
      "metadata": {
        "id": "i3ydN6dCU0R4",
        "outputId": "647137c1-193e-4ae6-ea79-946ee6203e91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-19 09:10:32--  https://raw.githubusercontent.com/binoydutt/Resume-Job-Description-Matching/refs/heads/master/data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 646072 (631K) [text/plain]\n",
            "Saving to: â€˜data.csvâ€™\n",
            "\n",
            "data.csv            100%[===================>] 630.93K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-10-19 09:10:32 (14.2 MB/s) - â€˜data.csvâ€™ saved [646072/646072]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load the Dataset\n",
        "#### ðŸ“Œ Dataset: A provided CSV file containing job descriptions from different industries (IT, Healthcare, Finance, Marketing, etc.).\n",
        "\n",
        "1. Download the dataset (link below).\n",
        "2. Load it into Python using Pandas.\n",
        "3. View the first few rows to understand its structure."
      ],
      "metadata": {
        "id": "OSp39kGdUDZp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ujju9awITxuH",
        "outputId": "21e32b08-0252-475a-cc46-a6b6b9ad5064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                          company  \\\n",
            "0           1          Visual BI Solutions Inc   \n",
            "1           2                       Jobvertise   \n",
            "2           3           Santander Consumer USA   \n",
            "3           4   Federal Reserve Bank of Dallas   \n",
            "4           5                           Aviall   \n",
            "\n",
            "                                            position  \\\n",
            "0  Graduate Intern (Summer 2017) - SAP BI / Big D...   \n",
            "1                          Digital Marketing Manager   \n",
            "2    Manager, Pricing Management Information Systems   \n",
            "3               Treasury Services Analyst Internship   \n",
            "4                              Intern, Sales Analyst   \n",
            "\n",
            "                                                 url     location  \\\n",
            "0  https://www.glassdoor.com/partner/jobListing.h...    Plano, TX   \n",
            "1  https://www.glassdoor.com/partner/jobListing.h...   Dallas, TX   \n",
            "2  https://www.glassdoor.com/partner/jobListing.h...   Dallas, TX   \n",
            "3  https://www.glassdoor.com/partner/jobListing.h...   Dallas, TX   \n",
            "4  https://www.glassdoor.com/partner/jobListing.h...   Dallas, TX   \n",
            "\n",
            "       headquaters                employees founded  \\\n",
            "0        Plano, TX      51 to 200 employees    2010   \n",
            "1  Berlin, Germany        1 to 50 employees    2011   \n",
            "2       Dallas, TX  5001 to 10000 employees    1995   \n",
            "3       Dallas, TX   1001 to 5000 employees    1914   \n",
            "4       Dallas, TX   1001 to 5000 employees  Boeing   \n",
            "\n",
            "                         industry  \\\n",
            "0          Information Technology   \n",
            "1                         Unknown   \n",
            "2                         Finance   \n",
            "3                         Finance   \n",
            "4  Subsidiary or Business Segment   \n",
            "\n",
            "                                     Job Description  \n",
            "0   Location: Plano, TX or Oklahoma City, OK Dura...  \n",
            "1   The Digital Marketing Manager is the front li...  \n",
            "2   Summary of Responsibilities:The Manager Prici...  \n",
            "3   ORGANIZATIONAL SUMMARY:   As part of the nati...  \n",
            "4     Aviall is the world's largest provider of n...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Preprocessing the Job Descriptions\n",
        "#### ðŸ“Œ Goal: Clean the text by removing stopwords, punctuation, and unnecessary characters.\n",
        "\n",
        "1. Use NLTK to tokenize the descriptions.\n",
        "2. Remove stopwords and special characters.\n",
        "3. Convert text to lowercase for consistency."
      ],
      "metadata": {
        "id": "jP9XyZzeULkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk -q\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')  # Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and special characters\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens if token not in stop_words and re.sub(r'[^a-zA-Z]', '', token)]\n",
        "    # Join tokens back to text\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['processed_description'] = df['Job Description'].apply(preprocess_text)\n",
        "\n",
        "print(df[['Job Description', 'processed_description']].head())"
      ],
      "metadata": {
        "id": "IgUGbR5nUKJ4",
        "outputId": "10e32249-3f05-47fa-9a94-f58f9818f02a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Job Description  \\\n",
            "0   Location: Plano, TX or Oklahoma City, OK Dura...   \n",
            "1   The Digital Marketing Manager is the front li...   \n",
            "2   Summary of Responsibilities:The Manager Prici...   \n",
            "3   ORGANIZATIONAL SUMMARY:   As part of the nati...   \n",
            "4     Aviall is the world's largest provider of n...   \n",
            "\n",
            "                               processed_description  \n",
            "0  location plano tx oklahoma city ok duration in...  \n",
            "1  digital marketing manager front line patient c...  \n",
            "2  summary responsibilities manager pricing mis r...  \n",
            "3  organizational summary part nation central ban...  \n",
            "4  aviall world s largest provider new aviation p...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Extract Skills Using Named Entity Recognition (NER)\n",
        "#### ðŸ“Œ Goal: Use spaCyâ€™s built-in NER to detect and extract skills from job descriptions.\n",
        "\n",
        "1. Load spaCyâ€™s English model.\n",
        "2. Use NER to identify important keywords.\n",
        "3. Extract words related to technical skills, tools, and expertise."
      ],
      "metadata": {
        "id": "rttovi6vUVX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy -q\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_skills(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    skills = []\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in [\"ORG\", \"PRODUCT\", \"SKILL\"] or any(keyword in ent.text.lower() for keyword in [\"skill\", \"tool\", \"expert\", \"experience\", \"knowledge\"]):\n",
        "            skills.append(ent.text)\n",
        "    return ', '.join(list(dict.fromkeys(skills)))\n",
        "\n",
        "df['extracted_skills'] = df['processed_description'].apply(extract_skills)\n",
        "\n",
        "print(df[['processed_description', 'extracted_skills']].head())"
      ],
      "metadata": {
        "id": "d31wh2yDUT9Y",
        "outputId": "e923b11d-9b91-4921-fde2-5e84c5d43154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               processed_description  \\\n",
            "0  location plano tx oklahoma city ok duration in...   \n",
            "1  digital marketing manager front line patient c...   \n",
            "2  summary responsibilities manager pricing mis r...   \n",
            "3  organizational summary part nation central ban...   \n",
            "4  aviall world s largest provider new aviation p...   \n",
            "\n",
            "                                    extracted_skills  \n",
            "0                                         gpa scores  \n",
            "1                                            digital  \n",
            "2                                                     \n",
            "3  federal reserve bank, dallas treasury services...  \n",
            "4                                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Identify the Most In-Demand Skills\n",
        "#### ðŸ“Œ Goal: Count the most frequently mentioned skills in job descriptions.\n",
        "\n",
        "1. Create a word frequency distribution of extracted skills.\n",
        "2. Identify the top 10 most required skills."
      ],
      "metadata": {
        "id": "TizfP0pyUanq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def get_skill_frequency(df):\n",
        "    all_skills = []\n",
        "    for skills in df['extracted_skills'].dropna():\n",
        "        skills_list = skills.split(', ')\n",
        "        all_skills.extend(skills_list)\n",
        "    skill_freq = Counter(all_skills)\n",
        "    return skill_freq\n",
        "\n",
        "skill_frequency = get_skill_frequency(df)\n",
        "\n",
        "\n",
        "top_10_skills = skill_frequency.most_common(10)\n",
        "\n",
        "\n",
        "print(\"Top 10 Most In-Demand Skills:\")\n",
        "for skill, count in top_10_skills:\n",
        "    print(f\"{skill}: {count} times\")\n",
        "\n",
        "df['skill_count'] = df['extracted_skills'].apply(lambda x: len(x.split(', ')) if pd.notna(x) else 0)"
      ],
      "metadata": {
        "id": "9SLRxwryUm4g",
        "outputId": "3d96b20d-d5ad-4972-fd61-e19d16a4c31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most In-Demand Skills:\n",
            ": 43 times\n",
            "microsoft: 34 times\n",
            "microsoft office: 23 times\n",
            "deloitte: 11 times\n",
            "texas usa: 7 times\n",
            "ibm: 7 times\n",
            "grant thornton international ltd one: 6 times\n",
            "united states: 6 times\n",
            "bachelor s master: 6 times\n",
            "deloitte university: 4 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Categorize Skills by Industry\n",
        "#### ðŸ“Œ Goal: Compare the most in-demand skills across different industries.\n",
        "\n",
        "1. Group job descriptions by industry.\n",
        "2. Extract and analyze skills for each industry.\n",
        "3. Compare IT vs. Marketing vs. Healthcare, etc.."
      ],
      "metadata": {
        "id": "3v8W0pneUoku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "grouped = df.groupby('industry')\n",
        "\n",
        "\n",
        "def analyze_industry_skills(group):\n",
        "    all_skills = []\n",
        "    for skills in group['extracted_skills'].dropna():\n",
        "        skills_list = skills.split(', ')\n",
        "        all_skills.extend(skills_list)\n",
        "    skill_freq = Counter(all_skills)\n",
        "    return skill_freq.most_common(5)\n",
        "\n",
        "industry_skills = {}\n",
        "for industry, group in grouped:\n",
        "    industry_skills[industry] = analyze_industry_skills(group)\n",
        "\n",
        "print(\"Top 5 Skills by Industry:\")\n",
        "for industry, skills in industry_skills.items():\n",
        "    print(f\"\\n{industry}:\")\n",
        "    for skill, count in skills:\n",
        "        print(f\"  {skill}: {count} times\")\n",
        "\n",
        "industries_to_compare = ['Information Technology', 'Unknown', 'Finance', 'Marketing']\n",
        "print(\"\\nComparison of Top Skills Across Selected Industries:\")\n",
        "for industry in industries_to_compare:\n",
        "    if industry in industry_skills:\n",
        "        print(f\"\\n{industry}:\")\n",
        "        for skill, count in industry_skills[industry][:3]:\n",
        "            print(f\"  {skill}: {count} times\")"
      ],
      "metadata": {
        "id": "HzZmaKoyUsSX",
        "outputId": "ebbc13c0-870e-4a5e-b4d5-587ef57456b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Skills by Industry:\n",
            "\n",
            "$500 million to $1 billion (USD) per year:\n",
            "  : 2 times\n",
            "\n",
            "Accounting & Legal:\n",
            "  deloitte: 10 times\n",
            "  grant thornton international ltd one: 6 times\n",
            "  deloitte university: 4 times\n",
            "  united states: 3 times\n",
            "  microsoft: 3 times\n",
            "\n",
            "Aerospace & Defense:\n",
            "  : 1 times\n",
            "  northrop: 1 times\n",
            "  jsp: 1 times\n",
            "  oracle sql server relational: 1 times\n",
            "\n",
            "Arts, Entertainment & Recreation:\n",
            "  penney: 4 times\n",
            "  microsoft: 2 times\n",
            "  android: 2 times\n",
            "\n",
            "Business Services:\n",
            "  : 9 times\n",
            "  ibm: 6 times\n",
            "  microsoft: 5 times\n",
            "  dmv: 2 times\n",
            "  k offer associates variety: 2 times\n",
            "\n",
            "Company - Public:\n",
            "  gm: 1 times\n",
            "\n",
            "Construction, Repair & Maintenance:\n",
            "  united states: 1 times\n",
            "  habitat council: 1 times\n",
            "  north america inc holcim us inc aggregate industries management inc affiliates: 1 times\n",
            "\n",
            "Finance:\n",
            "  : 6 times\n",
            "  federal reserve bank: 3 times\n",
            "  dallas treasury services department regularly apply analytical problem: 3 times\n",
            "  microsoft: 3 times\n",
            "  invesco ltd leading: 3 times\n",
            "\n",
            "Health Care:\n",
            "  : 8 times\n",
            "  phoenix house: 1 times\n",
            "  texas phoenix house: 1 times\n",
            "  phoenix house facebo phoenix house: 1 times\n",
            "  microsoft: 1 times\n",
            "\n",
            "Information Technology:\n",
            "  microsoft: 7 times\n",
            "  texas usa: 6 times\n",
            "  : 6 times\n",
            "  microsoft sql: 2 times\n",
            "  microsoft office: 2 times\n",
            "\n",
            "Insurance:\n",
            "  blue cross blue shield: 2 times\n",
            "  gpa greater: 2 times\n",
            "  hcsc intern: 2 times\n",
            "  gpa: 2 times\n",
            "  microsoft: 2 times\n",
            "\n",
            "Manufacturing:\n",
            "  microsoft: 4 times\n",
            "  : 3 times\n",
            "  tx department name ag: 2 times\n",
            "  ag: 2 times\n",
            "  tx department: 2 times\n",
            "\n",
            "Media:\n",
            "  microsoft office: 11 times\n",
            "  bachelor s master: 5 times\n",
            "  college university business economics: 2 times\n",
            "  microsoft: 1 times\n",
            "\n",
            "Mining & Metals:\n",
            "  united states: 1 times\n",
            "  habitat council: 1 times\n",
            "  north america inc holcim us inc aggregate industries management inc affiliates: 1 times\n",
            "  america s job exchange: 1 times\n",
            "\n",
            "Non-Profit:\n",
            "  intern research development: 1 times\n",
            "  american stroke association: 1 times\n",
            "  united states: 1 times\n",
            "\n",
            "Real Estate:\n",
            "  jll s management mortgage bankers underwriters daily: 1 times\n",
            "  microsoft: 1 times\n",
            "\n",
            "Retail:\n",
            "  : 1 times\n",
            "  digital marketing: 1 times\n",
            "  ford: 1 times\n",
            "  jc penney company inc plano: 1 times\n",
            "  jcpenneys desktop mobile application: 1 times\n",
            "\n",
            "Subsidiary or Business Segment:\n",
            "  microsoft office: 6 times\n",
            "  gpa education college coursework: 2 times\n",
            "  : 1 times\n",
            "  businessjob: 1 times\n",
            "\n",
            "Transportation & Logistics:\n",
            "  microsoft: 1 times\n",
            "\n",
            "Travel & Tourism:\n",
            "  microsoft: 1 times\n",
            "\n",
            "Unknown:\n",
            "  : 3 times\n",
            "  microsoft: 2 times\n",
            "  digital: 1 times\n",
            "  google: 1 times\n",
            "  metadata management data integration technologies hadoop: 1 times\n",
            "\n",
            "Unknown / Non-Applicable per year:\n",
            "  microsoft visio: 1 times\n",
            "  cms: 1 times\n",
            "  bachelor s master: 1 times\n",
            "\n",
            "Comparison of Top Skills Across Selected Industries:\n",
            "\n",
            "Information Technology:\n",
            "  microsoft: 7 times\n",
            "  texas usa: 6 times\n",
            "  : 6 times\n",
            "\n",
            "Unknown:\n",
            "  : 3 times\n",
            "  microsoft: 2 times\n",
            "  digital: 1 times\n",
            "\n",
            "Finance:\n",
            "  : 6 times\n",
            "  federal reserve bank: 3 times\n",
            "  dallas treasury services department regularly apply analytical problem: 3 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BoqkQtM82san",
        "outputId": "fc733452-33c1-43ef-c52f-a6eebdde7074",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "77jtIjyF2r9G"
      }
    }
  ]
}